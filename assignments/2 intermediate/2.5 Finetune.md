# 2.5 Finetune

## 1. 环境配置
按照要求，安装 `xtuner`, 这里是安装后的验证：

![xtuner cfg](../../assets/xtuner-cfg.png)

## 2. 修改数据
这里将 `尖米` 修改成 `Super Nova`, 具体代码参考[这里](./finetune/change_user.py)。
显示修改后的数据前3条：

![change user](../../assets/finetune_data.png)

## 3. 启动训练
### 3.1 修改配置和训练脚本

使用如下命令创建训练脚本：
``` bash
xtuner copy-cfg internlm2_5_chat_7b_qlora_alpaca_e3 ./
```

并且根据路径和数据，修改相应的代码。

### 3.2 启动Lora微调

``` bash
xtuner train ./config/internlm2_5_chat_7b_qlora_alpaca_e3_copy.py --deepspeed deepspeed_zero2 --work-dir ./work_dirs/assistTuner
```

需要等待一段时间完成训练， 然后可以在 `work_dirs/` 目录下查看训练结果：

![finetune result](../../assets/finetune_workdir.png)

这里记录一下训练参数的含义：
| 参数名                     | 解释                                                         |
| -------------------------- | ------------------------------------------------------------ |
| **data_path**              | 数据路径或 HuggingFace 仓库名                                |
| **max_length**             | 单条数据最大 Token 数，超过则截断                            |
| **pack_to_max_length**     | 是否将多条短数据拼接到 max_length，提高 GPU 利用率           |
| **accumulative_counts**    | 梯度累积，每多少次 backward 更新一次参数                     |
| **sequence_parallel_size** | 并行序列处理的大小，用于模型训练时的序列并行                 |
| **batch_size**             | 每个设备上的批量大小                                         |
| **dataloader_num_workers** | 数据加载器中工作进程的数量                                   |
| **max_epochs**             | 训练的最大轮数                                               |
| **optim_type**             | 优化器类型，例如 AdamW                                       |
| **lr**                     | 学习率                                                       |
| **betas**                  | 优化器中的 beta 参数，控制动量和平方梯度的移动平均           |
| **weight_decay**           | 权重衰减系数，用于正则化和避免过拟合                         |
| **max_norm**               | 梯度裁剪的最大范数，用于防止梯度爆炸                         |
| **warmup_ratio**           | 预热的比例，学习率在这个比例的训练过程中线性增加到初始学习率 |
| **save_steps**             | 保存模型的步数间隔                                           |
| **save_total_limit**       | 保存的模型总数限制，超过限制时删除旧的模型文件               |
| **prompt_template**        | 模板提示，用于定义生成文本的格式或结构                       |
| ......                     | ......                                                       |

### 3.3. 转换权重

只需要使用提供的`xtuner convert pth_to_hf`命令即可。
转换后得到`HuggingFace`格式的权重，可以直接上传到 `HuggingFace Hub`。
得到hf文件夹，包括以下文件：

![hf files](../../assets/finetune_hf.png)

### 3.4. 合并模型

上一步中的微调，并不是全参数微调，而是`LoRA`微调，得到的权重是所谓的`LoRA adapter`。
可以将其与原文件合并，从而得到完整的模型权重，进行推理。
使用`xtuner convert merge`命令即可。
得到的权重，包括以下文件：

![merge files](../../assets/finetune_merged.png)

## 4. 推理, 模型WebUI对话

推理可以使用提供的streamlit脚本，生成对话。

![webui](../../assets/finetune_webui.png)